# -*- coding: utf-8 -*-
"""wiki_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XRXQcIW-a3vzH_Ob47xYQCANE0T2vgyT
"""

import argparse
import requests
from bs4 import BeautifulSoup
import shlex

def scrape_wikipedia_page(url):
    """
    A helper function to scrape just the title and first paragraph of any Wikipedia page.
    We'll use this when we "follow" a link from the "See also" section.
    """
    try:
        # Send a request to the URL and get the HTML content
        response = requests.get(url)
        # This will raise an error if the request was not successful (e.g., 404 Not Found)
        response.raise_for_status()

        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the title using its unique HTML ID
        title = soup.find(id="firstHeading").text

        # Find the main content area
        content_div = soup.find(id="mw-content-text")
        # Find the very first <p> tag directly inside the content area
        first_paragraph_tag = content_div.find('p', recursive=False)
        # Get the text from the paragraph tag, or a default message if it's not found
        first_paragraph = first_paragraph_tag.text if first_paragraph_tag else "First paragraph not found."

        return title, first_paragraph

    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return None, None

def main():
    """The main function where our script's logic lives."""
    # Set up argparse to handle command-line arguments
    parser = argparse.ArgumentParser(description="Scrape a Wikipedia page for a programming language.")
    # We define one required argument: -p or --programming_language
    parser.add_argument('-p', '--programming_language', required=True, help='The name of the programming language.')

    # --- Google Colab Workaround ---
    # In a normal terminal, you'd run: python wiki_scraper.py -p "Python"
    # In Colab, we can't do that, so we simulate it with a string.
    # You can change "Python" to any other language to test!
    cmd_line_input = 'wiki_scraper.py -p "Java"'
    # shlex.split() correctly splits the string into a list that argparse can understand
    # We use [1:] to ignore the "wiki_scraper.py" part
    args = parser.parse_args(shlex.split(cmd_line_input)[1:])
    # --- End of Workaround ---

    language = args.programming_language
    # This formats the language name for a URL (e.g., turns "C++" into "C%2B%2B")
    formatted_language = requests.utils.quote(language)
    # Construct the final URL
    url = f"https://en.wikipedia.org/wiki/{formatted_language}_(programming_language)"

    print(f"Scraping data for '{language}' from URL: {url}\n")

    try:
        # Make the HTTP request to the URL
        response = requests.get(url)
        # Check if the page was successfully downloaded (status code 200 means OK)
        if response.status_code != 200:
            print(f"Error: Could not fetch the page. Status code: {response.status_code}")
            print(f"The page for '{language}' might not exist with the '(programming_language)' suffix.")
            return

        # Create the BeautifulSoup object to parse the HTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # --- 1. Information Extraction ---
        print("--- Main Article Information ---")
        title = soup.find(id="firstHeading").text
        print(f"Title: {title}\n")

        # The main content is in a div with this id. We find it first.
        content_div = soup.find(id="mw-content-text").find(class_='mw-parser-output')
        first_p = content_div.find('p', recursive=False)
        print("First Paragraph:")
        print(first_p.text.strip() if first_p else "Could not find the first paragraph.")
        print("-" * 30)

        # --- 2. Subheading Extraction ---
        print("\n--- Subheadings ---")
        # Find all heading tags from h2 to h6
        subheadings = content_div.find_all(['h2', 'h3', 'h4', 'h5', 'h6'])
        for heading in subheadings:
            # The .replace() cleans up the "[edit]" text that appears next to headings
            print(heading.text.replace('[edit]', '').strip())
        print("-" * 30)

        # --- 3. "See Also" Section Exploration ---
        print("\n--- 'See Also' Links ---")
        # Find the heading for the "See also" section by its ID
        see_also_heading = soup.find(id="See_also")
        if see_also_heading:
            # The links are in the <ul> (unordered list) that comes right after the heading
            link_list = see_also_heading.find_next('ul')
            see_also_links = []
            if link_list:
                # Find all <a> (link) tags within that list
                for a_tag in link_list.find_all('a', href=True):
                    # We only want internal Wikipedia links
                    if a_tag['href'].startswith('/wiki/'):
                        link_text = a_tag.text
                        full_url = "https://en.wikipedia.org" + a_tag['href']
                        see_also_links.append((link_text, full_url))
                        # Print the link with a number for the user to choose
                        print(f"[{len(see_also_links)}] {link_text}")

                # --- 4. Interactive Part: Follow a Link ---
                try:
                    choice_str = input("\nEnter the number of a link to follow (or just press Enter to skip): ")
                    if choice_str: # Check if the user actually typed something
                        choice = int(choice_str)
                        if 0 < choice <= len(see_also_links):
                            link_to_follow = see_also_links[choice - 1]
                            print(f"\nFetching content for: {link_to_follow[0]}...")
                            # Call our helper function to scrape the new page
                            new_title, new_first_p = scrape_wikipedia_page(link_to_follow[1])
                            if new_title and new_first_p:
                                print(f"\nTitle: {new_title}")
                                print(f"First Paragraph: {new_first_p.strip()}")
                        else:
                            print("Invalid number. Skipping.")
                    else:
                        print("Skipping.")
                except ValueError:
                    print("That wasn't a valid number. Skipping.")
        else:
            print("No 'See also' section found on this page.")

    except requests.exceptions.RequestException as e:
        print(f"A critical error occurred: {e}")

# This standard Python line ensures that the main() function is called only when the script is executed directly
if __name__ == '__main__':
    main()